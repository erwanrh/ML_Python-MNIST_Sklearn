\section{Part 2}

\subsection{Loss Function}
Our custom loss function is an accuracy score with a penalty on inter-class errors. If the predicted value is different than the true value then the error count will increase +1. We decided to separate the data into two classes : 
\begin{table}[hbt]
\centering
  \begin{tabular}{l|cccccccccc}
   Label  & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9  \\
    \hline
   Class & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 \\
  \end{tabular}
\end{table}

The counts for each class and label i are as follows : 

\begin{table}[hbt]
\centering
\makegapedcells
\begin{tabular}{cc|cc}
\multicolumn{2}{c}{}
            &   \multicolumn{2}{c}{Predicted} \\
    &       &   True &  False              \\ 
    \cline{2-4}
\multirow{2}{*}{\rotatebox[origin=c]{90}{Actual}}
    & True   & $tp_i$   & $fn_i$                 \\
    & False    & $fp_i$  & $tn_i$                \\ 
    \cline{2-4}
    \end{tabular}

\end{table}

We define the Recall score for one class or label i as : 
\begin{equation*}
	Recall_i = \frac{tp_i}{tp_i+fn_i}
\end{equation*}
which is the well classified individuals on the number of individuals actually in this class. We compute a weighted accuracy on the labels (0:9) and one on the classes (0-1). The formula we use is : 
\begin{equation*}
	Weighted\_Accuracy = {\sum^{K}_{i=1} Recall_i \over K}
\end{equation*}

We join both metrics by multiplying them to have a penalized weighted accuracy of the labels : 
\begin{equation*}
	Score = Weighted\_Accuracy_{Label}Â *  Weighted\_Accuracy_{Class}
\end{equation*}

\subsection{Pipeline}
To scales the features, we chose to use a MaxAbsScaler on the data because we want to minimize their variance knowing by constraining them to be between 0 and 1. In fact, we have 784 features which represent a color code that goes from 0 to 255 which can be easily scaled to a [0,1] interval without any loss of information. We trains a linear SVM model (using the SVC). 

\subsection{Algorithm}
The Gaussian Radial Basis Function, or Gaussian RBF uses a similarity function that measures how much each instance resembles a particular landmark. It will create a landmark at each instance of the dataset and calculate the similarity function :  $\varphi_\gamma(x,l)= exp (-\gamma ||x-l||^2)$.
It is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at the landmark). The idea is to transform the dataset and by doing so, make it linearly separable through non-linear methods (Figure 4).
The metrics of the distance is usually the euclidean.
However, using the Gaussian RBF can be computationally expensive especially for large datasets given it transforms the training dataset with m instances and n features into a m instances and m features dataset.
$\gamma$ and C both play the role of regularization hyperparameter. Increasing $\gamma$ make the bell-shap narrower so it reduces the instance's range of influence which makes the decision boundary ends up being more irregular, wiggling around individual instances.
If the model is overfitting, trying to diminish the value of $\gamma$ could solve the problem.