\section{Part 1}

\subsection{Cross-Validation with GridSearchCV}
\textbf{Explain in your report what happens when we run clf.fit(X\_train, Y\_train)} \\

The line \verb|clf.fit(X\_train, Y\_train)| here uses the fit method on the object  clf and takes as parameters the labels and features of the train sample. The fit method is training the model defined in the clf object. The object clf is from the class GridSearchCV which allows us to find the best hyperparameters among a fixed list we chose and perform a cross-validation. It is taking as parameters an object we §named knn of the class KNeighborsClassifier(), a dictionary named parameters containing the number of neighbors to be tested in the knn algorithm (1 to 5 here) and the cv parameter referring to the number of folds to be used in the cross-validation. Basically it will perform a 3-folds cross-validation on a kNN model with 1 to 5 neighbors on the train sample and it will allow us to keep the best model. The kNN algorithm is parametered with the default metric which is the Euclidean distance : $\sqrt{\sum^n_{i=1}(x_i - y_i)^2}$. The functions are all part of the sklearn package. \\

 \textbf{What is the complexity for each of the three following cases?} \\

Complexity can be divided into two kinds of complexity i.e: 1) time complexity, deal with how long the algorithm is executed, and 2) space complexity, deal with how much memory is used by this algorithm.
\begin{table}[ht]
		\caption{Complexity}
		\vspace{0.5cm}
		\centering
		\begin{tabular}{|c|c|c|c|c|c|c|c|}
			\hline
			& kNN & Linear SVC & Log Reg   \\  [0.3ex]
			\hline 
			Time Training    & O(n*k*d)       &  O(m*n)     & O(n*d)        \\ 
			\hline 
			Space  &  O(n*d)      &  O(l)     &  O(d)      \\ 
			\hline 
		\end{tabular} 
        \label{table:nonlin}
	\end{table}	

With n : size of the training sample, d : dimension of the data, k : number of neighbors, m : number of features, l : support vectors. \\

\textbf{What is the test accuracy? What would be the accuracy of random guess?} \\

The test accuracy is the measure of how often the points are correctly classified in the test sample. In our case the accuracy is 0.875.  It means that 87.5\% of the time, the points are correctly classified on the test sample. It is computed as the number of well classified individuals over the sample size. If we did a random guess we would randomly choose an output in the range 0 to 9 so the accuracy would converge towards $\frac{1}{10}$ according to the LLN.  \\


\textbf{What is LinearSVC() classifier? Which kernel are we using? What is C? (this is a tricky question, try to find the answer online )}\\

LinearSVC means Linear Support Vector Classification, which is supervised learning methods used for classification. LinearSVC are classes capable of performing binary and multi-class classification on a dataset. This classifier is trained to separate the True labels from the False labels with a boundary compute with a kernel function. In our multi-class problem, the algorithm creates one classifier for each class and performs a “one-vs-rest” training by training the data of one class versus all the the other data as one other class. 
\\
The linear SVC is using a linear kernel function of the form : $\langle x, x'\rangle$ which implies a boundary of linear form\footnote{Appendix \ref{appendix:part1svm}, figure \ref{fig:svmex}}.\\ The parameter C is the regularisation parameter. It is a parameter applied to the squared hinge loss\footnote{This is true for Linear SVC. Non-linear SVC will use the l2 distance.} distance: $\max(0, 1-y\cdot y')^2$ and it allows to control the importance given to good classification (accuracy) when increasing its value with respect to the margin size. When decreasing its value the accuracy is decreased but it increases the importance of a larger margin between data points and thus a simpler decision function. \\

\textbf{What is the outcome of np.logspace(-8, 8, 17, base=2)? More generally, what is the outcome of np.logspace(-a, b, k, base=m)?}\\

The outcome of np.logspace(-8, 8, 17, base=2) is a logarithmic space going from $2^{-8}$ to $2^8$ with 17 numbers equally spaced on log scale.
 The logspace function from the numpy package will return k numbers going from $m^{-a}$ to $m^b$ spaced on a log scale with a log base m. \\

\textbf{What is the meaning of the warnings? What is the parameter responsible for its appearance?}\\

The warning tells us that the algorithm did not converge, it did not reach the stop criterion\footnote{The stop criterion here is the tol parameter} before the number of maximum iterations. The parameter responsible for its appearance is the max\_iter parameter. Its value is not large enough for the algorithm to converge. The data variance is maybe too large for the algorithm to efficiently perform the SVM. \\

\textbf{What did we change with respect to the previous run of LinearSVC()?} \\

We added a pipeline which is a method of the ScikitLearn package that allows to streamline the data pre-processing. In the pipeline we added a \verb|MaxAbsScaler()| method to scale the absolute data between 0 and 1 and thus reduce the variance of the data. We notice that the algorithm is not showing a convergence warning. \\

\textbf{Explain what happens if we execute :} 
\begin{verbatim}pipe.fit(X_train, y_train)
pipe.predict(X_test, y_test)\end{verbatim} \\
The first will execute the pipeline defined with a \verb|MaxAbsScaler|  preprocessing on the features and fit a SVM but this time with no C parameter defined which will be 1.0 by default. The second line is returning an error. Indeed, the predict method is returning an array of predictions made with the pipe object and should only take one argument, that is an array of features (X). Here we put two arguments as parameters and it is the reason why it is returning an error.  \\

\textbf{What is the difference between} \verb|StandardScaler()| and \verb|MaxAbsScaler()|? \textbf{What are other scaling options available in sklearn? }\\

StandardScaler will standardize the data : $\frac{x-m}{\sigma}$ with m the mean and $\sigma$ the standard deviation of data. It differs from MaxAbsScaler  because in this case we map the absolute value of data in a [0,1] range.\\	

Two of the other scaling options available in sklearn are : \\
- \verb|MinMaxScaler| which transforms features by scaling each feature to a given range [min, max]. \\
- \verb|RobustScaler| which is used if the data contains many outliers. It basically removes the data median and then scales the data according to the quantile range which is the range between the 1st quartile(25\%) and the 3rd quartile (75\%). \\ 

\textbf{Using the previous code as an example achieve test accuracy  $\geq0.9$ . You can use any method from sklearn package. Give a mathematical description of the selected method. Explain the range of considered hyper-parameters.}\\

We tried the Random Forest algorithm which is a method creating a fixed number of random trees (CART algorithm). The randomness in this algorithm comes with the selection of features used to create the trees. Each tree is created with a fixed number of features but these features are randomly drawn from the whole range of available features. \\

In our case, the dataset has 784 features and the algorithm choses $\sqrt{784} = 28$ features for each tree. This function also uses the bagging method for the elements of the sample. It means that for each tree it takes a random sample of the same size as the initial sample. In this case we fit a train sample of size 2000 so the bootstrap bags will have 2000 random elements (they can appear multiple times). In each tree the method is to successively split the features into 2 groups. The choice of the feature and threshold for the split is made by minimising a criterion : the gini coefficient or the entropy. In our case we put both hyper-parameters for the Grid Search to find the best one. \\

We used the method \verb|RandomForestClassifier()| from the skLearn package in the pipeline along with a \verb|StandardScaler| preprocessing on features to normalise the data and reduce the variance so we avoid divergence of the algorithm. The number of trees to generate and the split quality criterion are the two hyper-parameters we chose to exploit. The default number of trees is 100 so we tried with 50 and 150. We used the accuracy scoring for the grid search and a 3 folds cross-validation. This configuration resulted in an accuracy $>0.9$.

\subsection{Visualising errors}
The error in the chunk of code was because the \verb|predict_proba| method returns an array of probabilities within an array. We must then pick the first element of the array (index 0) to retrieve the probabilities array\footnote{See Jupyter Notebook File}. The output of this code chunk is a figure with the 4 first misclassified items by the algorithm along with the probability of each class.  \\

\subsection{Changing the loss function}

\textbf{What is balanced\_accuracy\_score? Write its mathematical mathematical description.} \\

The balanced accuracy in binary and multi-class classification problems is used to deal with imbalanced datasets. 
For a binary classification is defined as the arithmetic mean of the sensitivity (also called recall or true positive rate) and the specificity (also called true negative rate). As a consequence, for a multi-class classification, it represents the average recall per class. The recall score for one class i :
\begin{equation*} recall_i = {tp_i\over tp_i + fp_i}\end{equation*} 
with $tp_i$: true positive (well classified items in the class i) and $fn_i$: false negative (items which should be classified in class i but were not).\\
Instead of calculating the regular score which is $tp+tn \over sampleSize$, the balanced score is, for K classes, :  
\begin{equation*}
	\sum^K_{i=1}recall_i \over K
\end{equation*}
If the number in each category of prediction is the same, regular score = balanced score. Otherwise, the good predictions of an over represented class will not inflate the balanced score unlike the regular one.\\


\textbf{What is the confusion matrix? What are the conclusions that we can draw from the} \verb|confusion_matrix(y_test, clf4.predict(X_test))?| \\

The general idea is to count the number of instances of class A that are classified as class B by computing a matrix giving information about true class and predicted class. For example, to know the number of times the classifier confused images of 5s with 2s, you would look in the 5th row and 2nd column of the confusion matrix. The row is the actual class and the column is the predicted class given by the algorithm. \\

As we can see in our case\footnote{Appendix \ref{appendix:changingloss}, figure \ref{fig:confusion}}, 8s are often confused with 5s (3/17=18\% of the time when the actual class is 8) and 3s are also confused with 5s 13\% of the time (3/23). Also, 5s are detected only 57\% (8/14) of the time. 0s and 9s seem well detected with respectively 100\% (22/22) and 92\% (24/26) recall/true positive rate.\\

Regarding the scores, the balanced is slightly inferior to the regular one (83\% vs 84\%) due to the underrepresentation of the worst predicted class (ie 5s).Because there are several classes, it could be interesting to transform the confusion matrix into a heat map. On the heat map we can check that the algorithm is good at predicting classes since the brighter cells are on the main diagonal. Even though, 5s are darker than other classes explained by the under-representation of the class and a few errors. 1s are well predicted given its bright square on the main diagonal but it can be partly explained by the over-representation of 1s in the dataset. 


