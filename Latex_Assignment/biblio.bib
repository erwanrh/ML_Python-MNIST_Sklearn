
@book{geron_hands-machine_2019,
	address = {Sebastopol, CA},
	title = {Hands-on machine learning with {Scikit}-{Learn}, {Keras}, and {TensorFlow}: concepts, tools, and techniques to build intelligent systems},
	isbn = {978-1-4920-3264-9},
	shorttitle = {Hands-on machine learning with {Scikit}-{Learn}, {Keras}, and {TensorFlow}},
	abstract = {Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. The updated edition of this best-selling book uses concrete examples, minimal theory, and two production-ready Python frameworks-Scikit-Learn and TensorFlow 2-to help you gain an intuitive understanding of the concepts and tools for building intelligent systems. Practitioners will learn a range of techniques that they can quickly put to use on the job. Part 1 employs Scikit-Learn to introduce fundamental machine learning tasks, such as simple linear regression. Part 2, which has been significantly updated, employs Keras and TensorFlow 2 to guide the reader through more advanced machine learning methods using deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started. NEW FOR THE SECOND EDITION:Updated all code to TensorFlow 2Introduced the high-level Keras APINew and expanded coverage including TensorFlow's Data API, Eager Execution, Estimators API, deploying on Google Cloud ML, handling time series, embeddings and more With Early Release ebooks, you get books in their earliest form-the author's raw and unedited content as he or she writes-so you can take advantage of these technologies long before the official release of these titles. You'll also receive updates when significant changes are made, new chapters are available, and the final ebook bundle is released.},
	language = {English},
	publisher = {O'Reilly Media, Inc.},
	author = {Géron, Aurélien},
	year = {2019},
	note = {OCLC: 1135343456},
}

@misc{tyler_how_nodate,
	title = {How to classify {MNIST} digits with different neural network architectures {\textbar} by {Tyler} {Elliot} {Bettilyon} {\textbar} {Teb}’s {Lab} {\textbar} {Medium}},
	url = {https://medium.com/tebs-lab/how-to-classify-mnist-digits-with-different-neural-network-architectures-39c75a0f03e3},
	urldate = {2020-11-01},
	author = {Tyler, Elliot},
	annote = {Fully connected neural networks
Loss of information because we're not looking at which pixel is next to which by flattening the picture in one vector.
CNNs conventional neural networks maintain these spacial relationships (designed for computer vision).
 
Loss function : categorical cross entropy En regardant les données comme une matrice one hot encoded : chaque catégorie est une colonne contennant 1 ou 0. (softmax)
vera
 
Optimization algorithm : stochastic gradient descent},
	file = {How to classify MNIST digits with different neural network architectures | by Tyler Elliot Bettilyon | Teb’s Lab | Medium:/Users/erwanrahis/Zotero/storage/BD8SWYHP/how-to-classify-mnist-digits-with-different-neural-network-architectures-39c75a0f03e3.html:text/html},
}

@misc{noauthor_understanding_nodate,
	title = {Understanding {Categorical} {Cross}-{Entropy} {Loss}, {Binary} {Cross}-{Entropy} {Loss}, {Softmax} {Loss}, {Logistic} {Loss}, {Focal} {Loss} and all those confusing names},
	url = {https://gombru.github.io/2018/05/23/cross_entropy_loss/},
	urldate = {2020-11-01},
	file = {Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names:/Users/erwanrahis/Zotero/storage/LD7AWT4A/cross_entropy_loss.html:text/html},
}
