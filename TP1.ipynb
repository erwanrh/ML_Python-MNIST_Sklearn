{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> TP1 : Basic functions for Supervised Machine Learning. </center></h1>\n",
    "\n",
    "The deadline for report submission is Tuesday, November 10th 2020.\n",
    "\n",
    "Note: the goal of this first TP is to become familiar with 'sklearn' class in Python. In particular, we introduce most popular supervised learning algorithms. \n",
    "\n",
    "PART 1 is a list of commands that should be followed step by step. PART 2 is an open problem for which we are waiting for your creativity!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, make_scorer, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PART 1 -- MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of TP1 we pursue the following goals:\n",
    "1. Apply standard ML algorithms on a standard benchmark data\n",
    "2. Learn basic means of data visualizations\n",
    "3. Get familiar with sklearn's GridSearchCV and Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset consists of black and white images of hand-written digits from $0$ to $9$ of size $28 \\times 28$.\n",
    "In this exercise we will work with a small from the original MNIST dataset. \n",
    "\n",
    "If you are interested in the whole dataset, execute the following commands\n",
    "```python\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home=custom_data_home)\n",
    "```\n",
    "\n",
    "Hence, the observations $(X_1, Y_1), \\ldots, (X_n, Y_n)$ are such that $X_i \\in \\mathbb{R}^{784}$ and $Y_i \\in \\{0, \\ldots, 9\\}$. To be more precise, each component of vector $X_i$ is a number between $0$ and $255$, which signifies the intensity of black color.\n",
    "\n",
    "The initial goal is to build a classifier $\\hat g$, which receives a new image $X$ and outputs the number that is present on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/mnist1_features_train.npy', allow_pickle=True)\n",
    "y_train = np.load('data/mnist1_labels_train.npy', allow_pickle=True)\n",
    "X_test = np.load('data/mnist1_features_test.npy', allow_pickle=True)\n",
    "y_test = np.load('data/mnist1_labels_test.npy', allow_pickle=True)\n",
    "\n",
    "n_samples, n_features = X_train.shape # extract dimensions of the design matrix\n",
    "print('Train data contains: {} samples of dimension {}'.format(n_samples, n_features))\n",
    "print('Test data contains: {} samples'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each observation is actually an image, we can visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plt.subplots(1, 10)[1]  # creates a grid of 10 plots\n",
    "\n",
    "# More details about zip() function here https://docs.python.org/3.3/library/functions.html#zip\n",
    "images_and_labels = list(zip(X_train, y_train)) \n",
    "for ax, (image, label) in zip(axes, images_and_labels[:10]):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape((28, 28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('{}'.format(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('Number of {}s in the train dataset is {}'.format(i, np.sum([y_train == str(i)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we conclude that the dataset is rather balanced, that is, each class contains similar amount of observations. The rarest class is $y = 6$ with $175$ examples and the most common class is $y = 2$ with $226$ examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question:** Explain in your report what happens when we run \n",
    "```python\n",
    "clf.fit(X_train, y_train)\n",
    "```\n",
    "What is the complexity for each of the three following cases? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV with kNN : a simple baseline\n",
    "knn = KNeighborsClassifier() # defining classifier\n",
    "parameters = {'n_neighbors': [1, 2, 3, 4, 5]} # defining parameter space\n",
    "clf = GridSearchCV(knn, parameters, cv=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Returned hyperparameter: {}'.format(clf.best_params_))\n",
    "print('Best classification accuracy in train is: {}'.format(clf.best_score_))\n",
    "print('Classification accuracy on test is: {}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the test accuracy? What would be the accuracy of random guess?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question:** What is ``` LinearSVC()``` classifier? Which kernel are we using? What is ```C```? (this is a tricky question, try to find the answer online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the outcome of ```np.logspace(-8, 8, 17, base=2)```? More generally, what is the ourcome of ```np.logspace(-a, b, k, base=m)```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier\n",
    "svc = LinearSVC(max_iter=5000)\n",
    "parameters2 = {'C': np.logspace(-8, 8, 17, base=2)} # defining parameter space\n",
    "clf2 = GridSearchCV(svc, parameters2, cv=3)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Returned hyperparameter: {}'.format(clf2.best_params_))\n",
    "print('Best classification accuracy in train is: {}'.format(clf2.best_score_))\n",
    "print('Classification accuracy on test is: {}'.format(clf2.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What is the meaning of the warnings? What is the parameter responsible for its appearence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier + Pipeline\n",
    "pipe = Pipeline([('scaler', MaxAbsScaler()), ('svc', svc)])\n",
    "parameters3 = {'svc__C': np.logspace(-8, 8, 17, base=2)} # defining parameter space\n",
    "clf3 = GridSearchCV(pipe, parameters3, cv=3)\n",
    "clf3.fit(X_train, y_train)\n",
    "\n",
    "print('Returned hyperparameter: {}'.format(clf3.best_params_))\n",
    "print('Best classification accuracy in train is: {}'.format(clf3.best_score_))\n",
    "print('Classification accuracy on test is: {}'.format(clf3.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What did we change with respect to the previous run of ```LinearSVC()```?\n",
    "\n",
    "**Question:** Explain what happens if we execute\n",
    "```python\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pipe.predict(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('logreg', LogisticRegression(max_iter=5000))])\n",
    "parameters4 = {'logreg__C': np.logspace(-8, 8, 17, base=2)} # defining parameter space\n",
    "clf4 = GridSearchCV(pipe, parameters4, cv=3)\n",
    "clf4.fit(X_train, y_train)\n",
    "\n",
    "print('Returned hyperparameter: {}'.format(clf4.best_params_))\n",
    "print('Best classification accuracy in train is: {}'.format(clf4.best_score_))\n",
    "print('Classification accuracy on test is: {}'.format(clf4.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** what is the difference between ```StandardScaler()``` and ```MaxAbsScaler()```? What are other scaling options available in ```sklearn```?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** using the previous code as an example achieve test accuracy $\\geq 0.9$. You can use any method from sklearn package. Give a mathematical description of the selected method. Explain the range of considered hyperparamers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest classifier for accuracy >0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "pipe_test = Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier())])\n",
    "parameters_test = {'rf__n_estimators':[100,150],\n",
    "                   'rf__criterion': ['gini','entropy']} \n",
    "scoring_test = {'accuracy': make_scorer(accuracy_score)}\n",
    "clf_test = GridSearchCV(pipe_test, parameters_test, cv=3,scoring = scoring_test, refit='accuracy')\n",
    "clf_test.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Returned hyperparameter: {}'.format(clf_test.best_params_))\n",
    "print('Best classification accuracy in train is: {}'.format(clf_test.best_score_))\n",
    "print('Classification accuracy on test is: {}'.format(clf_test.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ```sklearn``` methods are able to output probabilities ```predict_proba(X_test)```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** There is a mistake in the following chunk of code. Fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "axes = plt.subplots(2, 4)[1]  # creates a grid of 10 plots\n",
    "\n",
    "# More details about zip() function here https://docs.python.org/3.3/library/functions.html#zip\n",
    "y_pred = clf4.predict(X_test)\n",
    "j = 0 # Index which iterates over plots\n",
    "for true_label, pred_label, image in list(zip(y_test, y_pred, X_test)):\n",
    "    if j == 4: # We only want to look at 4 first mistakes\n",
    "        break\n",
    "    if true_label != pred_label:\n",
    "        # Plotting predicted probabilities\n",
    "        axes[1, j].bar(np.arange(10), clf4.predict_proba(image.reshape(1, -1))[0]) \n",
    "        axes[1, j].set_xticks(np.arange(10))\n",
    "        axes[1, j].set_yticks([])\n",
    "        \n",
    "        # Plotting the image\n",
    "        axes[0, j].imshow(image.reshape((28, 28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        axes[0, j].set_xticks([])\n",
    "        axes[0, j].set_yticks([])\n",
    "        axes[0, j].set_title('Predicted {}'.format(pred_label)+'/True {}'.format(true_label),fontsize=8)\n",
    "        j += 1\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It often happens that the accuracy is not the right way to evaluate the performance. ```sklearn``` has a large variety of other metrics both in classification and regression. See https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "Here we want to understand how to change the cross-validation metric with minimal effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classifier + Pipeline + New score function\n",
    "\n",
    "pipe = Pipeline([('scaler', MaxAbsScaler()), ('svc', svc)])\n",
    "parameters4 = {'svc__C': np.logspace(-8, 8, 17, base=2)} # defining parameter space\n",
    "balanced_scorer = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "clf4 = GridSearchCV(pipe, parameters3, cv=3, scoring=balanced_scorer)\n",
    "clf4.fit(X_train, y_train)\n",
    "\n",
    "print('Returned hyperparameter: {}'.format(clf4.best_params_))\n",
    "print('Best Balanced accuracy in train is: {}'.format(clf4.best_score_))\n",
    "print('Balanced accuracy on test is: {}'.format(clf4.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is ```balanced_accuracy_score```? Write its mathematical mathematical description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sometimes it is important to look at the confusion matrix of the prediction.\n",
    "\n",
    "**Question:** What is the confusion matrix? What are the conclusions that we can draw from the ```confusion_matrix(y_test, clf4.predict(X_test))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, clf4.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(clf4, X_test, y_test, cmap =plt.get_cmap('Greens').reversed() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2 -- Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we have contains images with $10$ classes. Normally, accuracy is a reasonable choice of the loss function to be optimized, but in this problem we *really* do not like when digits from $\\{5, 6, 7, 8, 9\\}$ are predicted to be from $\\{0, 1, 2, 3, 4\\}$.\n",
    "\n",
    "**Question:** Propose a loss function that would address our needs. Explain your choice.\n",
    "\n",
    "**Question:** Following above examples, make an ML pipeline that uses *your* loss function and finds appropriate classifiers.\n",
    "\n",
    "When writing your report on this part, include:\n",
    "   1. description of your loss function\n",
    "   2. description of the pipeline\n",
    "   3. description of the algorithms that you used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas \n",
    "- Loss function : Weighted accuracy, inter-class penalty\n",
    "- Pipeline : One hot encoding (gérer les catégories comme 1 / 0) + Standard scale on features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOM SCORE \n",
    "#Définition d'une fonction de perte\n",
    "def custom_MNISTscorer(y_true, y_predict):\n",
    "    #Classe 1 : les chiffres de 0 à 4, Classe 0 : les chiffres de 5 à 9\n",
    "    y_predict_class = (np.array(y_predict).astype(int)<5).astype(int)\n",
    "    y_true_class = (np.array(y_true).astype(int)<5).astype(int)\n",
    "    \n",
    "    # Pénalité de 1 si les prédictions n'appartiennent pas à la bonne classe\n",
    "    Class_weightedaccuracy = balanced_accuracy_score(y_true_class, y_predict_class)\n",
    "    Label_weightedaccuracy = balanced_accuracy_score(y_true, y_predict)\n",
    "    \n",
    "    return Label_weightedaccuracy * Class_weightedaccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "pipe = Pipeline([('Scaler', MaxAbsScaler()),('rf', RandomForestClassifier())])\n",
    "\n",
    "custom_scorer = make_scorer(custom_MNISTscorer)\n",
    "\n",
    "parameters_test = {'rf__n_estimators': [100,150,300], 'rf__criterion': ['gini', 'entropy']}\n",
    "\n",
    "clf5 = GridSearchCV(pipe, parameters_test, cv=3, scoring=custom_scorer)\n",
    "clf5.fit(X_train, y_train)\n",
    "\n",
    "print('Returned hyperparameter: {}'.format(clf5.best_params_))\n",
    "print('Best Balanced accuracy in train is: {}'.format(clf5.best_score_))\n",
    "print('Balanced accuracy on test is: {}'.format(clf5.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe = Pipeline([('scaler', MaxAbsScaler()), ('svc', SVC(max_iter=5000))])\n",
    "custom_scorer = make_scorer(custom_MNISTscorer)\n",
    "\n",
    "parameters_test3 = {'svc__kernel': ['rbf'], 'svc__C': np.logspace(-8, 8, 5, base=2)}\n",
    "clf7 = GridSearchCV(pipe, parameters_test3, cv=3, scoring=custom_scorer)\n",
    "clf7.fit(X_train, y_train)\n",
    "\n",
    "print('Returned hyperparameter: {}'.format(clf7.best_params_))\n",
    "print('Best Balanced accuracy in train is: {}'.format(clf7.best_score_))\n",
    "print('Balanced accuracy on test is: {}'.format(clf7.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(clf7, X_test, y_test, cmap=\"Blues\")\n",
    "plt.savefig('confusion_mat.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = clf7.predict(X_test)\n",
    "custom_MNISTscorer(y_test, pred_test)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
